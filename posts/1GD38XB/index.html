<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Scrapy爬虫配置 | 胡言胡语</title><meta name="keywords" content="java,python,selenium"><meta name="author" content="言小妹"><meta name="copyright" content="言小妹"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1. 开发环境1.1 需要安装如下必需的开发包：pipenv,类库包如下123pip install pipenvpip install scrapy scrapyd-clientpip install requests pymysql beautifulsoup4 lxml js2py selenium     数据库主机、数据库名称、用户名、密码等信息在settings.py文件中配置； 配置"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy爬虫配置"><meta property="og:url" content="https://seniortesting.club/posts/1GD38XB/index.html"><meta property="og:site_name" content="胡言胡语"><meta property="og:description" content="1. 开发环境1.1 需要安装如下必需的开发包：pipenv,类库包如下123pip install pipenvpip install scrapy scrapyd-clientpip install requests pymysql beautifulsoup4 lxml js2py selenium     数据库主机、数据库名称、用户名、密码等信息在settings.py文件中配置； 配置"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414200916.jpg"><meta property="article:published_time" content="2021-04-14T12:11:56.866Z"><meta property="article:modified_time" content="2021-04-14T12:11:56.866Z"><meta property="article:author" content="言小妹"><meta property="article:tag" content="java"><meta property="article:tag" content="python"><meta property="article:tag" content="selenium"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414200916.jpg"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://seniortesting.club/posts/1GD38XB/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"top-center"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2021-04-14 20:11:56"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)});const t=saveToLocal.get("aside-status");void 0!==t&&("hide"===t?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const o=saveToLocal.get("global-font-size");void 0!==o&&document.documentElement.style.setProperty("--global-font-size",o+"px")})(window)</script><style>.card-announcement .social-button{margin:.6rem 0 0 0;text-align:center}.card-announcement .social-button a{display:block;margin:.2rem 0;background-color:var(--btn-bg);color:var(--btn-color);line-height:1.6rem;transition:all .3s;position:relative;z-index:1}</style><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="胡言胡语" type="application/atom+xml"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">59</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">72</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-file-archive"></i> <span>课程系列</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/%E7%A8%8B%E5%BA%8F%E8%AF%AD%E8%A8%80/Java/java%E5%9F%BA%E7%A1%80/"><i class="fa-fw fas fa-coffee"></i> <span>java基础系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/%E6%8E%A5%E5%8F%A3%E8%87%AA%E5%8A%A8%E5%8C%96/junit5%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-thumbs-up"></i> <span>junit5系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/%E6%8E%A5%E5%8F%A3%E8%87%AA%E5%8A%A8%E5%8C%96/pytest%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-balance-scale"></i> <span>pytest系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/UI%E8%87%AA%E5%8A%A8%E5%8C%96/cypress%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-graduation-cap"></i> <span>cypress系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/UI%E8%87%AA%E5%8A%A8%E5%8C%96/playwright%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-wrench"></i> <span>playwright系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/UI%E8%87%AA%E5%8A%A8%E5%8C%96/puppeteer%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-gem"></i> <span>puppeteer系列</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i> <span>找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">胡言胡语</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-file-archive"></i> <span>课程系列</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/%E7%A8%8B%E5%BA%8F%E8%AF%AD%E8%A8%80/Java/java%E5%9F%BA%E7%A1%80/"><i class="fa-fw fas fa-coffee"></i> <span>java基础系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/%E6%8E%A5%E5%8F%A3%E8%87%AA%E5%8A%A8%E5%8C%96/junit5%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-thumbs-up"></i> <span>junit5系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/%E6%8E%A5%E5%8F%A3%E8%87%AA%E5%8A%A8%E5%8C%96/pytest%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-balance-scale"></i> <span>pytest系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/UI%E8%87%AA%E5%8A%A8%E5%8C%96/cypress%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-graduation-cap"></i> <span>cypress系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/UI%E8%87%AA%E5%8A%A8%E5%8C%96/playwright%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-wrench"></i> <span>playwright系列</span></a></li><li><a class="site-page child" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/UI%E8%87%AA%E5%8A%A8%E5%8C%96/puppeteer%E7%B3%BB%E5%88%97/"><i class="fa-fw fas fa-gem"></i> <span>puppeteer系列</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i> <span>找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Scrapy爬虫配置<a class="post-edit-link" href="https://github.com/seniortesting/seniortesting.club/edit/main/source/_posts/爬虫/python-scrapy环境配置.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon fas fa-history"></i><span class="post-meta-label">更新于</span><time datetime="2021-04-14T12:11:56.866Z" title="undefined 2021-04-14 20:11:56">2021-04-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Scrapy爬虫配置"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/1GD38XB/#post-comment"><span id="twikoo-count"></span></a></span></div></div></div><article class="post-content" id="article-container"><h2 id="1-开发环境"><a href="#1-开发环境" class="headerlink" title="1. 开发环境"></a>1. 开发环境</h2><h3 id="1-1-需要安装如下必需的开发包：pipenv-类库包如下"><a href="#1-1-需要安装如下必需的开发包：pipenv-类库包如下" class="headerlink" title="1.1 需要安装如下必需的开发包：pipenv,类库包如下"></a>1.1 需要安装如下必需的开发包：<code>pipenv</code>,类库包如下</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install pipenv</span><br><span class="line">pip install scrapy scrapyd-client</span><br><span class="line">pip install requests pymysql beautifulsoup4 lxml js2py selenium  </span><br></pre></td></tr></table></figure><ul><li>数据库主机、数据库名称、用户名、密码等信息在<code>settings.py</code>文件中配置；</li><li>配置好数据库后，cmd进入程序所在目录，运行`scrapy crawl 项目名称即可；</li><li>使用<code>scrapyd-client</code>中的<code>scrapyd-deploy</code>命令行工具可以将脚本部署到指定的<code>scrapyd</code>服务上；</li></ul><h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><p>1.创建项目</p><p>在开始爬取之前，我们必须创建一个新的Scrapy项目，我这里命名为jianshu_article。打开Mac终端，cd到你打算存储代码的目录中，运行下列命令:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 安装scrapy</span><br><span class="line">pip install scrapy</span><br><span class="line">scrapy --<span class="built_in">help</span></span><br><span class="line">//Mac终端运行如下命令：</span><br><span class="line">scrapy startproject spider_pingbook</span><br></pre></td></tr></table></figure><p>2.创建爬虫程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//cd到上面创建的文件目录</span><br><span class="line">cd spider_pingbook</span><br><span class="line">//创建爬虫程序</span><br><span class="line">scrapy genspider jianshu jianshu.com</span><br><span class="line">/*</span><br><span class="line">文件说明：</span><br><span class="line">  scrapy.cfg  项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）</span><br><span class="line">  items.py    设置数据存储模型，用于结构化数据，如：Django的Model</span><br><span class="line">  pipelines    数据处理行为，如：一般结构化的数据持久化</span><br><span class="line">  settings.py 配置文件，如：USER_AGENT(模拟浏览器，应对网站反爬)，递归的层数、并发数，延迟下载等</span><br><span class="line">  spiders      爬虫目录，如：创建文件，编写爬虫规则</span><br><span class="line">*/</span><br></pre></td></tr></table></figure><p>为了方便编写程序，我们用Pycharm打开项目，执行完上面的命令程序会自动创建目录及文件，其中生成了一个jianshu.py的文件，后面我们主要逻辑都将写在此文件中。</p><p>3.设置数据模型<br>双击items.py文件。<br>找到你想爬取的简书作者首页，如我自己的首页<a href="https://www.jianshu.com/u/6b14223f1b58" target="_blank" rel="noopener external nofollow noreferrer">https://www.jianshu.com/u/6b14223f1b58</a>，用谷歌浏览器打开，空白处鼠标右击，单击“检查”进入控制台开发者模式：<br>通过分析网页源码，我们大概需要这些内容：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuArticalItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    avatar = scrapy.Field()     <span class="comment">#头像</span></span><br><span class="line">    nickname = scrapy.Field()      <span class="comment">#昵称</span></span><br><span class="line">    time = scrapy.Field()   <span class="comment">#发表时间</span></span><br><span class="line">    wrap_img = scrapy.Field()   <span class="comment">#封面（缺省值）</span></span><br><span class="line">    title = scrapy.Field()     <span class="comment">#标题</span></span><br><span class="line">    abstract = scrapy.Field()   <span class="comment">#正文部分显示</span></span><br><span class="line">    read = scrapy.Field()   <span class="comment">#查看人数</span></span><br><span class="line">    comments = scrapy.Field()   <span class="comment">#评论数</span></span><br><span class="line">    like = scrapy.Field()   <span class="comment">#喜欢（点赞）</span></span><br><span class="line">    detail = scrapy.Field()   <span class="comment">#文章详情url</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如此数据模型就创建好了，后面运行爬虫的时候，我得到的数据将存进模型对应的位置。</p><p>4.分析网页源码，编写爬虫<br>推荐一款chrome的xpath自动选择生成工具<a href="https://www.crx4chrome.com/extensions/ljngjbnaijcbncmcnjfhigebomdlkcjo/" target="_blank" rel="noopener external nofollow noreferrer">ChroPath</a>这里给出XPath表达式的例子及对应的含义:</p><ul><li><code>/html/head/title</code>: 选择HTML文档中<head>标签内的<title>元素</title></head></li><li><code>/html/head/title/text()</code>: 选择上面提到的<title>元素的文字</title></li><li><code>//td</code> <code>//li</code>: 选择所有的<td>元素</td></li><li><code>//div[@class=&quot;mine&quot;]</code>: 选择所有具有 class=”mine” 属性的 div 元素</li></ul><p>上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。 如果您想了解的更多，我们推荐 这篇XPath教程 。<br>通过上面的介绍，相信你可以做接下来的爬虫工作了，下面贴上jianshu.py的全部代码，以供参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> jianshu_article.items <span class="keyword">import</span> JianshuArticleItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;jianshu&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;jianshu.com&#x27;</span>]</span><br><span class="line">    user_id = <span class="string">&quot;1b4c832fb2ca&quot;</span> <span class="comment">#替换此用户ID可获取你需要的数据，或者放开下一行的注释</span></span><br><span class="line">    <span class="comment">#user_id = input(&#x27;请输入作者id：\n&#x27;)</span></span><br><span class="line">    url = <span class="string">&quot;https://www.jianshu.com/u/&#123;0&#125;?page=1&quot;</span>.<span class="built_in">format</span>(user_id)</span><br><span class="line">    start_urls = [</span><br><span class="line">        url,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># [关注,粉丝,文章]</span></span><br><span class="line">        a = response.xpath(<span class="string">&#x27;//div[@class=&quot;main-top&quot;]/div[@class=&quot;info&quot;]/ul/li/div/a/p/text()&#x27;</span>).extract()</span><br><span class="line">        <span class="built_in">print</span>(a)</span><br><span class="line">        <span class="comment"># [字数,收获喜欢]</span></span><br><span class="line">        b = response.xpath(<span class="string">&#x27;//div[@class=&quot;main-top&quot;]/div[@class=&quot;info&quot;]/ul/li/div/p/text()&#x27;</span>).extract()</span><br><span class="line">        <span class="built_in">print</span>(b)</span><br><span class="line">        <span class="comment"># 大头像</span></span><br><span class="line">        c = response.xpath(<span class="string">&#x27;//div[@class=&quot;main-top&quot;]/a[@class=&quot;avatar&quot;]/img/@src&#x27;</span>).extract_first()</span><br><span class="line">        <span class="built_in">print</span>(c)</span><br><span class="line">        <span class="comment"># 用户名</span></span><br><span class="line">        d = response.xpath(<span class="string">&#x27;//div[@class=&quot;main-top&quot;]/div[@class=&quot;title&quot;]/a/text()&#x27;</span>).extract_first()</span><br><span class="line">        <span class="built_in">print</span>(d)</span><br><span class="line">        <span class="comment"># 性别</span></span><br><span class="line">        e = response.xpath(<span class="string">&#x27;//div[@class=&quot;main-top&quot;]/div[@class=&quot;title&quot;]/i/@class&#x27;</span>).extract_first()</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取文章总数，计算页数。（简书网站默认每页是9组数据）</span></span><br><span class="line">        temp = <span class="built_in">int</span>(a[<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">if</span> (temp % <span class="number">9</span> &gt; <span class="number">0</span>):</span><br><span class="line">            count = temp // <span class="number">9</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            count = temp // <span class="number">9</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;总共&quot;</span> + <span class="built_in">str</span>(count) + <span class="string">&quot;页&quot;</span>)</span><br><span class="line"></span><br><span class="line">        base_url = <span class="string">&quot;https://www.jianshu.com/u/&#123;0&#125;?page=&#123;1&#125;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, count + <span class="number">1</span>):</span><br><span class="line">            i = count + <span class="number">1</span> - i  <span class="comment">#理论上正序1~count就是按顺序获取的，但是获取的数据是倒置的，所以我们获取count~1的数据，得到的数组就是按照网页形式1~count页码排序的了</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(base_url.<span class="built_in">format</span>(self.user_id, i), dont_filter=<span class="literal">True</span>, callback=self.parse_page)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#迭代返回每页的内容</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//div[@id=&quot;list-container&quot;]/ul/li&#x27;</span>):</span><br><span class="line">            item = JianshuArticleItem()</span><br><span class="line">            item[<span class="string">&#x27;wrap_img&#x27;</span>] = sel.xpath(<span class="string">&#x27;a/img/@src&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;avatar&#x27;</span>] = sel.xpath(<span class="string">&#x27;div//a[@class=&quot;avatar&quot;]/img/@src&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;nickname&#x27;</span>] = sel.xpath(<span class="string">&#x27;div//a[@class=&quot;nickname&quot;]/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;time&#x27;</span>] = sel.xpath(<span class="string">&#x27;div//span[@class=&quot;time&quot;]/@data-shared-at&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = sel.xpath(<span class="string">&#x27;div/a[@class=&quot;title&quot;]/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;abstract&#x27;</span>] = sel.xpath(<span class="string">&#x27;div/p[@class=&quot;abstract&quot;]/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;read&#x27;</span>] = sel.xpath(<span class="string">&#x27;div/div[@class=&quot;meta&quot;]/a[1]/text()&#x27;</span>).extract()[<span class="number">1</span>]</span><br><span class="line">            item[<span class="string">&#x27;comments&#x27;</span>] = sel.xpath(<span class="string">&#x27;div/div[@class=&quot;meta&quot;]/a[2]/text()&#x27;</span>).extract()[<span class="number">1</span>]</span><br><span class="line">            item[<span class="string">&#x27;like&#x27;</span>] = sel.xpath(<span class="string">&#x27;div/div[@class=&quot;meta&quot;]/span/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;detail&#x27;</span>] = sel.xpath(<span class="string">&#x27;div/a[@class=&quot;title&quot;]/@href&#x27;</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">至此爬虫代码编写完毕，如果要把获取的数据保存下来，你可以终端执行如下命令：</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">此命令用于把爬取的数据保存为json文件格式，当然你也可以保存为别的文件格式。</span><br><span class="line">Scrapy官方列出的文件格式有如下几种：(<span class="string">&#x27;json&#x27;</span>, <span class="string">&#x27;jsonlines&#x27;</span>, <span class="string">&#x27;jl&#x27;</span>, <span class="string">&#x27;csv&#x27;</span>, <span class="string">&#x27;xml&#x27;</span>, <span class="string">&#x27;marshal&#x27;</span>, <span class="string">&#x27;pickle&#x27;</span>)。</span><br><span class="line">温馨提示：如果要再次爬取，最好换一个文件名或者清空数据再爬取，因为第二还是写入上一个文件，数据不会覆盖，</span><br><span class="line">会堆积在上次获取的下面，造成json文件格式报错。</span><br><span class="line">*/</span><br><span class="line">scrapy crawl jianshu -o data.json</span><br></pre></td></tr></table></figure><p>程序执行完后，我们可以在文件目录看到新生成的data.json文件，双击可以看到我们要获取的全部数据：</p><p>如果需要存放到数据库中，需要注释<code>settings.py</code>文件中<code>ITEM_PIPELINES</code>的如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;spider_pingbook.pipelines.SpiderPingbookPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2-配置对应的scrapy-cfg文件中的scrapyd服务器-如果使用下面的gerapy则不需要配置这个部分-该部分主要是为了scrapyd-deploy使用"><a href="#1-2-配置对应的scrapy-cfg文件中的scrapyd服务器-如果使用下面的gerapy则不需要配置这个部分-该部分主要是为了scrapyd-deploy使用" class="headerlink" title="1.2 配置对应的scrapy.cfg文件中的scrapyd服务器(如果使用下面的gerapy则不需要配置这个部分),该部分主要是为了scrapyd-deploy使用"></a>1.2 配置对应的<code>scrapy.cfg</code>文件中的<code>scrapyd</code>服务器(如果使用下面的gerapy则不需要配置这个部分),该部分主要是为了<code>scrapyd-deploy</code>使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[deploy:cvr_news]</span><br><span class="line"># url &#x3D; http:&#x2F;&#x2F;localhost:6800&#x2F;</span><br><span class="line"># project &#x3D; spider_test</span><br><span class="line"># username &#x3D; deployer</span><br><span class="line"># password &#x3D; eaafbbdbe1494810b48a90651xe3452cd95f</span><br></pre></td></tr></table></figure><h2 id="2-分布式脚本执行部署环境"><a href="#2-分布式脚本执行部署环境" class="headerlink" title="2. 分布式脚本执行部署环境"></a>2. 分布式脚本执行部署环境</h2><blockquote><blockquote><blockquote><p>参考文档: <a href="https://blog.csdn.net/qq_38003892/article/details/80427278" target="_blank" rel="noopener external nofollow noreferrer">scrapy部署， Gerapy 分布式爬虫管理部署使用</a></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>参考文档： <a href="https://www.cnblogs.com/sxqjava/p/10037731.html" target="_blank" rel="noopener external nofollow noreferrer">gerapy+scrapyd组合管理分布式爬虫</a></p></blockquote></blockquote></blockquote><h3 id="2-1-gerapy服务器环境配置"><a href="#2-1-gerapy服务器环境配置" class="headerlink" title="2.1 gerapy服务器环境配置"></a>2.1 <code>gerapy</code>服务器环境配置</h3><ul><li>安装<code>scrapy</code>部署服务,也是一个远程服务。是运行<code>scrapy</code>爬虫的服务端程序,它支持以<code>http</code>接口命令方式发布、删除、启动、停止爬虫程序。</li><li>在电脑任意位置新建一个文件夹,打开cmd，进入到这个文件夹下，输入命令<code>gerapy init</code>.安装<code>Gerapy</code>.<br>初始化完成后会生成一个文件夹<code>gerapy</code>，该文件夹下面会生成一个<code>projects</code>文件夹.进入到该创建的<code>gerapy</code>文件夹下，再输入<code>gerapy migrate</code>完成<code>gerapy</code>初始化工作. 将scrapy脚本项目放到<code>projects</code>目录下,利用<code>gerapy runserver</code>，启动<code>gerapy</code>. 刷新即可看到部署的脚本。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ mkdir spider-gerapy</span><br><span class="line">$ cd spider-gerapy</span><br><span class="line"></span><br><span class="line"># 注意如果gerapy环境和执行及其scrpayd在同一个机器，他们需要配置不同的python virtualenv,因为</span><br><span class="line"># gerapy需要的pymysql&#x3D;&#x3D;0.7.10，但是这个版本的pymysql不兼容MySQL8，将会导致utf8mb4字符串不支持，出现： KeyError: 255</span><br><span class="line">$ pipenv shell</span><br><span class="line">$ pipenv install gerapy</span><br><span class="line"></span><br><span class="line">$ gerapy init</span><br><span class="line">$ cd gerapy</span><br><span class="line">$ gerapy migrate </span><br><span class="line">初始化数据库</span><br><span class="line">$ gerapy createsuperuser</span><br><span class="line">创建超级用户，用于登录界面</span><br><span class="line">$ gerapy runserver</span><br><span class="line">后台静默运行gerapy服务,注意一定要切换到新创建的gerapy目录下面</span><br><span class="line">$ nohup gerapy runserver 0.0.0.0:6000 &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">复制对应的脚本到服务端目录 projects</span><br><span class="line">$ cd gerapy</span><br><span class="line">$ mkdir projects</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="安装问题"><a href="#安装问题" class="headerlink" title="安装问题"></a><strong>安装问题</strong></h4><ol><li>无法安装<code>gevent</code>,直接下载编译好的安装包： <a href="https://www.lfd.uci.edu/~gohlke/pythonlib" target="_blank" rel="noopener external nofollow noreferrer">https://www.lfd.uci.edu/~gohlke/pythonlib</a></li></ol><h3 id="2-2-scrapyd脚本执行机器环境配置"><a href="#2-2-scrapyd脚本执行机器环境配置" class="headerlink" title="2.2 scrapyd脚本执行机器环境配置"></a>2.2 <code>scrapyd</code>脚本执行机器环境配置</h3><ul><li><code>scrapyd</code>不需要设置目录，可以同时管理多个爬虫,每个爬虫还可以有多个版本：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install scrapyd</span></span><br><span class="line">一般安装在类似目录： `/usr/local/lib/python3.9/site-packages/scrapyd`</span><br><span class="line"><span class="meta">$</span><span class="bash"> find / -name <span class="string">&quot;default_scrapyd.conf&quot;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nano default_scrapyd.conf</span></span><br><span class="line">1. 修改scrapyd服务的端口号，默认端口是`6800`</span><br><span class="line">2. 设置远程访问端口可以放行（`bind_address`由`127.0.0.1`改成0.0.0.0）</span><br><span class="line">3. 修改日志存放目录</span><br></pre></td></tr></table></figure><p>上面提到配置，需要修改的配置文件内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">eggs_dir    = /www/spider/eggs</span><br><span class="line">logs_dir    = /www/spider/logs</span><br><span class="line">items_dir   = /www/spider/items</span><br><span class="line"></span><br><span class="line">bind_address = 0.0.0.0</span><br><span class="line">http_port   = 6800</span><br><span class="line"></span><br><span class="line">username = deployertester</span><br><span class="line">password = eaatestf%5dz</span><br></pre></td></tr></table></figure><ul><li>上面的配置修改成功后执行以下脚本运行<code>scrapyd</code>后台启动服务：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup scrapyd &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><ul><li>脚本运行端需要安装对应的<code>scrapy</code>开发环境中提到的所有库，执行如下命令安装:</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy requests pymysql beautifulsoup4 lxml js2py selenium   </span><br></pre></td></tr></table></figure><p>所有的安装包默认安装在目录: <code>/usr/local/lib/python3.8/site-packages</code></p><h2 id="问题整理"><a href="#问题整理" class="headerlink" title="问题整理"></a>问题整理</h2><h2 id="Error-Keyerror-255-when-executing-pymysql-connect"><a href="#Error-Keyerror-255-when-executing-pymysql-connect" class="headerlink" title="Error Keyerror 255 when executing pymysql.connect"></a>Error Keyerror 255 when executing pymysql.connect</h2><p>原因是在进行连接的时候选择的字符与服务器的字符不一致，连接的使用使用的是<code>utf8</code>,而实际mysql8使用的字符是: <code>utf8mb4</code>.<br>需要执行如下两步操作:</p><ol><li><p>需要修改下面的参数: <code>charset=&quot;utf8mb4&quot;</code>. 如下:<br><img src="https://raw.githubusercontent.com/alterhu2020/StorageHub/master/img/20200730215024-2020-07-30.png" alt="20200730215024-2020-07-30"></p></li><li><p>升级对应的<code>pymysql</code>到最新的版本</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ pip install --upgrade pymysql</span><br><span class="line">或者</span><br><span class="line">$ pip install &#39;pymysql&gt;&#x3D;0.10.0&#39; --force-reinstall</span><br></pre></td></tr></table></figure><h3 id="scrapyd客户端日志查找"><a href="#scrapyd客户端日志查找" class="headerlink" title="scrapyd客户端日志查找"></a>scrapyd客户端日志查找</h3><p>查看对应的scrapyd的配置中的log文件目录是: <code>/www/spider/logs</code>,然后进入查看对应的日志</p><h3 id="在gerapy安装中安装的lxml会出现错误：-make-sure-the-development-packages-of-libxml2-and-libxslt-are-installed"><a href="#在gerapy安装中安装的lxml会出现错误：-make-sure-the-development-packages-of-libxml2-and-libxslt-are-installed" class="headerlink" title="在gerapy安装中安装的lxml会出现错误： make sure the development packages of libxml2 and libxslt are installed"></a>在<code>gerapy</code>安装中安装的lxml会出现错误： make sure the development packages of libxml2 and libxslt are installed</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libxml2-dev libxslt-dev</span><br></pre></td></tr></table></figure><h3 id="执行gerapy-init命令出现错误"><a href="#执行gerapy-init命令出现错误" class="headerlink" title="执行gerapy init命令出现错误"></a>执行<code>gerapy init</code>命令出现错误</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># gerapy init</span><br><span class="line">:0: UserWarning: You do not have a working installation of the service_identity module: &#39;cannot import name &#39;opentype&#39;&#39;.  Please install it from &lt;https:&#x2F;&#x2F;pypi.python.org&#x2F;pypi&#x2F;service_identity&gt; and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate&#x2F;hostname mappings may be rejected.</span><br><span class="line">Initialized workspace gerapy</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>原因是：本机上的service_identity模块太老旧，而通过install安装的时候不会更新到最新版本<br>解决方法：</p><ul><li><p>强制升级,执行命令: <code>pip install service_identity --force --upgrade</code></p></li><li><p>或者是找到最新版的安装包进行手动安装，最新包下载地址: <code>https://pypi.org/project/service_identity/#files</code>,下载对应的whl文件安装即可。</p></li></ul><h3 id="执行pipenv-install出现错误：ImportError-cannot-import-name-‘Mapping’-from-‘collections’"><a href="#执行pipenv-install出现错误：ImportError-cannot-import-name-‘Mapping’-from-‘collections’" class="headerlink" title="执行pipenv install出现错误：ImportError: cannot import name ‘Mapping’ from ‘collections’"></a>执行<code>pipenv install</code>出现错误：ImportError: cannot import name ‘Mapping’ from ‘collections’</h3><p>原因是执行的包错误，重新安装即可</p><h3 id="在gerapy中添加机器报错-the-JSON-object-must-be-str-not-39-bytes-39"><a href="#在gerapy中添加机器报错-the-JSON-object-must-be-str-not-39-bytes-39" class="headerlink" title="在gerapy中添加机器报错: the JSON object must be str, not &#39;bytes&#39;"></a>在gerapy中添加机器报错: <code>the JSON object must be str, not &#39;bytes&#39;</code></h3><p>可能是对应的scrapyd服务没有启动</p><h3 id="执行scrapyd命令出错-Failed-to-load-application-No-module-named-39-sqlite3-39"><a href="#执行scrapyd命令出错-Failed-to-load-application-No-module-named-39-sqlite3-39" class="headerlink" title="执行scrapyd命令出错: Failed to load application: No module named &#39;_sqlite3&#39;"></a>执行scrapyd命令出错: <code>Failed to load application: No module named &#39;_sqlite3&#39;</code></h3><p>原因是python采用编译安装的，导致没有加载对应的sqlite模块，重新编译安装加载sqlite模块，命令如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --enable-optimizations --enable-ipv6 --enable-loadable-sqlite-extensions</span><br></pre></td></tr></table></figure><h3 id="执行pip-V出错-ModuleNotFoundError-No-module-named-39-pip-internal-cli-main-39"><a href="#执行pip-V出错-ModuleNotFoundError-No-module-named-39-pip-internal-cli-main-39" class="headerlink" title="执行pip -V出错: ModuleNotFoundError: No module named &#39;pip._internal.cli.main&#39;"></a>执行<code>pip -V</code>出错: <code>ModuleNotFoundError: No module named &#39;pip._internal.cli.main&#39;</code></h3><p>解决方法，修复pip，执行命令: <code>python -m pip install --upgrade pip</code>，或者如下命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py</span><br><span class="line">python3 get-pip.py --force-reinstall</span><br></pre></td></tr></table></figure><h3 id="执行命令-sudo-add-apt-repository-ppa-deadsnakes-ppa-报错：add-apt-repository-gpg-keyserver-receive-failed-No-dirmngr-执行如下命令安装：dirmngr"><a href="#执行命令-sudo-add-apt-repository-ppa-deadsnakes-ppa-报错：add-apt-repository-gpg-keyserver-receive-failed-No-dirmngr-执行如下命令安装：dirmngr" class="headerlink" title="执行命令: sudo add-apt-repository ppa:deadsnakes/ppa ,报错：add-apt-repository gpg: keyserver receive failed: No dirmngr,执行如下命令安装：dirmngr"></a>执行命令: <code>sudo add-apt-repository ppa:deadsnakes/ppa</code> ,报错：<code>add-apt-repository gpg: keyserver receive failed: No dirmngr</code>,执行如下命令安装：dirmngr</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install dirmngr</span><br></pre></td></tr></table></figure><p>8.执行<code>apt update</code>命令报错:<code>Updating from such a repository can&#39;t be done securely, and is therefore disabled by default</code>,执行如下命令更新包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update --allow-unauthenticated</span><br></pre></td></tr></table></figure><h3 id="安装scrapy中twisted安装报错"><a href="#安装scrapy中twisted安装报错" class="headerlink" title="安装scrapy中twisted安装报错"></a>安装scrapy中twisted安装报错</h3><p>解决方法，切换到目录： <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted%EF%BC%8C%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94%E7%9A%84whl" target="_blank" rel="noopener external nofollow noreferrer">https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted，直接下载对应的whl</a>包<br>执行命令: <code>pip install Twisted‑20.3.0‑cp38‑cp38‑win32.whl</code></p><h3 id="如何设置scrapy的默认的user-agent和proxy代理"><a href="#如何设置scrapy的默认的user-agent和proxy代理" class="headerlink" title="如何设置scrapy的默认的user-agent和proxy代理"></a>如何设置scrapy的默认的user-agent和proxy代理</h3><p>在脚本目录下方有一个配置文件: <code>settings.py</code>, 如下配置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">  <span class="string">&#x27;spider_yanzhi.middlewares.UserAgentMiddleware&#x27;</span>: <span class="number">401</span>,</span><br><span class="line">  <span class="string">&#x27;spider_yanzhi.middlewares.CookiesMiddleware&#x27;</span>: <span class="number">402</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二个参数可以参考<code>DOWNLOADER_MIDDLEWARES_BASE</code>里面的默认数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#x27;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#x27;</span>: <span class="number">350</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#x27;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.retry.RetryMiddleware&#x27;</span>: <span class="number">550</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#x27;</span>: <span class="number">560</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#x27;</span>: <span class="number">580</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#x27;</span>: <span class="number">590</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#x27;</span>: <span class="number">600</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#x27;</span>: <span class="number">700</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#x27;</span>: <span class="number">750</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.stats.DownloaderStats&#x27;</span>: <span class="number">850</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#x27;</span>: <span class="number">900</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined" rel="external nofollow noreferrer">言小妹</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://seniortesting.club/posts/1GD38XB/">https://seniortesting.club/posts/1GD38XB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://seniortesting.club" target="_blank">胡言胡语</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/java/">java</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/selenium/">selenium</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414200916.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-6t+ed+2i-1n-4w" data-ad-client="ca-pub-1893384651266286" data-ad-slot="2627967536"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/14FD638/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414195222.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Debian MYSQL8安装及其环境配置</div></div></a></div><div class="next-post pull-right"><a href="/posts/38S8WFE/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414195016.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">scrapy logging模块配置</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/7670b080/" title="gradle工具整理"><img class="cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/girl12.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-26</div><div class="title">gradle工具整理</div></div></a></div><div><a href="/posts/2SXCPP6/" title="Docker容器化技术"><img class="cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414200558.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-14</div><div class="title">Docker容器化技术</div></div></a></div><div><a href="/posts/17V5PGF/" title="树莓派Linux安装及其环境配置"><img class="cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414194951.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-14</div><div class="title">树莓派Linux安装及其环境配置</div></div></a></div><div><a href="/posts/2DF9TDW/" title="Windows10机器新环境配置"><img class="cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414200916.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-14</div><div class="title">Windows10机器新环境配置</div></div></a></div><div><a href="/posts/2DYGSQC/" title="JDK版本新特性"><img class="cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/20210414195106.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-14</div><div class="title">JDK版本新特性</div></div></a></div><div><a href="/posts/2S29MCV/" title="Debian Linux命令其环境配置"><img class="cover" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/girl18.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-14</div><div class="title">Debian Linux命令其环境配置</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/img/blog/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"><div class="author-info__name">言小妹</div><div class="author-info__description">非学无以广才,非志无以成学</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">59</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">72</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/alterhu2020"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/seniortesting/seniortesting.club" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:alterhu2020@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83"><span class="toc-number">1.</span> <span class="toc-text">1. 开发环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E9%9C%80%E8%A6%81%E5%AE%89%E8%A3%85%E5%A6%82%E4%B8%8B%E5%BF%85%E9%9C%80%E7%9A%84%E5%BC%80%E5%8F%91%E5%8C%85%EF%BC%9Apipenv-%E7%B1%BB%E5%BA%93%E5%8C%85%E5%A6%82%E4%B8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 需要安装如下必需的开发包：pipenv,类库包如下</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.</span> <span class="toc-text">操作步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E9%85%8D%E7%BD%AE%E5%AF%B9%E5%BA%94%E7%9A%84scrapy-cfg%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84scrapyd%E6%9C%8D%E5%8A%A1%E5%99%A8-%E5%A6%82%E6%9E%9C%E4%BD%BF%E7%94%A8%E4%B8%8B%E9%9D%A2%E7%9A%84gerapy%E5%88%99%E4%B8%8D%E9%9C%80%E8%A6%81%E9%85%8D%E7%BD%AE%E8%BF%99%E4%B8%AA%E9%83%A8%E5%88%86-%E8%AF%A5%E9%83%A8%E5%88%86%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%BA%E4%BA%86scrapyd-deploy%E4%BD%BF%E7%94%A8"><span class="toc-number">1.3.</span> <span class="toc-text">1.2 配置对应的scrapy.cfg文件中的scrapyd服务器(如果使用下面的gerapy则不需要配置这个部分),该部分主要是为了scrapyd-deploy使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%86%E5%B8%83%E5%BC%8F%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83"><span class="toc-number">2.</span> <span class="toc-text">2. 分布式脚本执行部署环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-gerapy%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 gerapy服务器环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.1.</span> <span class="toc-text">安装问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-scrapyd%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 scrapyd脚本执行机器环境配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">问题整理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Error-Keyerror-255-when-executing-pymysql-connect"><span class="toc-number">4.</span> <span class="toc-text">Error Keyerror 255 when executing pymysql.connect</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapyd%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A5%E5%BF%97%E6%9F%A5%E6%89%BE"><span class="toc-number">4.1.</span> <span class="toc-text">scrapyd客户端日志查找</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8gerapy%E5%AE%89%E8%A3%85%E4%B8%AD%E5%AE%89%E8%A3%85%E7%9A%84lxml%E4%BC%9A%E5%87%BA%E7%8E%B0%E9%94%99%E8%AF%AF%EF%BC%9A-make-sure-the-development-packages-of-libxml2-and-libxslt-are-installed"><span class="toc-number">4.2.</span> <span class="toc-text">在gerapy安装中安装的lxml会出现错误： make sure the development packages of libxml2 and libxslt are installed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cgerapy-init%E5%91%BD%E4%BB%A4%E5%87%BA%E7%8E%B0%E9%94%99%E8%AF%AF"><span class="toc-number">4.3.</span> <span class="toc-text">执行gerapy init命令出现错误</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cpipenv-install%E5%87%BA%E7%8E%B0%E9%94%99%E8%AF%AF%EF%BC%9AImportError-cannot-import-name-%E2%80%98Mapping%E2%80%99-from-%E2%80%98collections%E2%80%99"><span class="toc-number">4.4.</span> <span class="toc-text">执行pipenv install出现错误：ImportError: cannot import name ‘Mapping’ from ‘collections’</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8gerapy%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%9C%BA%E5%99%A8%E6%8A%A5%E9%94%99-the-JSON-object-must-be-str-not-39-bytes-39"><span class="toc-number">4.5.</span> <span class="toc-text">在gerapy中添加机器报错: the JSON object must be str, not &#39;bytes&#39;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cscrapyd%E5%91%BD%E4%BB%A4%E5%87%BA%E9%94%99-Failed-to-load-application-No-module-named-39-sqlite3-39"><span class="toc-number">4.6.</span> <span class="toc-text">执行scrapyd命令出错: Failed to load application: No module named &#39;_sqlite3&#39;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8Cpip-V%E5%87%BA%E9%94%99-ModuleNotFoundError-No-module-named-39-pip-internal-cli-main-39"><span class="toc-number">4.7.</span> <span class="toc-text">执行pip -V出错: ModuleNotFoundError: No module named &#39;pip._internal.cli.main&#39;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4-sudo-add-apt-repository-ppa-deadsnakes-ppa-%E6%8A%A5%E9%94%99%EF%BC%9Aadd-apt-repository-gpg-keyserver-receive-failed-No-dirmngr-%E6%89%A7%E8%A1%8C%E5%A6%82%E4%B8%8B%E5%91%BD%E4%BB%A4%E5%AE%89%E8%A3%85%EF%BC%9Adirmngr"><span class="toc-number">4.8.</span> <span class="toc-text">执行命令: sudo add-apt-repository ppa:deadsnakes&#x2F;ppa ,报错：add-apt-repository gpg: keyserver receive failed: No dirmngr,执行如下命令安装：dirmngr</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85scrapy%E4%B8%ADtwisted%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99"><span class="toc-number">4.9.</span> <span class="toc-text">安装scrapy中twisted安装报错</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEscrapy%E7%9A%84%E9%BB%98%E8%AE%A4%E7%9A%84user-agent%E5%92%8Cproxy%E4%BB%A3%E7%90%86"><span class="toc-number">4.10.</span> <span class="toc-text">如何设置scrapy的默认的user-agent和proxy代理</span></a></li></ol></li></ol></div></div><div class="card-widget ads-wrap"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1893384651266286" data-ad-slot="3377657013" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 言小妹</div><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a><a style="margin-inline:5px" target="_blank" href="https://www.jsdelivr.com/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a><a style="margin-inline:5px" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>document.getElementsByClassName("mermaid").length&&(window.mermaidJsLoad?mermaid.init():getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(()=>{window.mermaidJsLoad=!0,mermaid.initialize({theme:"default"})}))</script><script>(()=>{const t=document.getElementById("twikoo-count"),o=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"twikoo-comment-7gxq358sd1f410ac",region:"ap-shanghai"},null))},n=()=>{twikoo.getCommentsCount({envId:"twikoo-comment-7gxq358sd1f410ac",region:"ap-shanghai",urls:[window.location.pathname],includeReply:!1}).then((function(o){t.innerText=o[0].count})).catch((function(t){console.error(t)}))},e=(e=!1)=>{"object"==typeof twikoo?(o(),e&&t&&setTimeout(n,0)):getScript("https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js").then(()=>{o(),e&&t&&setTimeout(n,0)})};e(!0)})()</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/css/butterfly.css"><script data-pjax src="https://cdn.jsdelivr.net/gh/alterhu2020/CDN/js/butterfly.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>